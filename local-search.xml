<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>FFmpeg开发——深入理解pts，dts和timebase</title>
    <link href="/2021/08/14/FFmpeg%E5%BC%80%E5%8F%91%E2%80%94%E2%80%94%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3pts%EF%BC%8Cdts%E5%92%8Ctimebase/"/>
    <url>/2021/08/14/FFmpeg%E5%BC%80%E5%8F%91%E2%80%94%E2%80%94%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3pts%EF%BC%8Cdts%E5%92%8Ctimebase/</url>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文将以具体视频播放器开发过程中遇到的具体问题，来系统地阐释pts，dts和timebase的概念。</p><h1 id="1-时间基"><a href="#1-时间基" class="headerlink" title="1.时间基"></a>1.时间基</h1><p>在FFmpeg开发中，经常会遇到结构体中有time_base这个成员，通过头文件查看他的类型是AVRational</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">AVRational</span>&#123;</span><br>    <span class="hljs-keyword">int</span> num; <span class="hljs-comment">///&lt; numerator</span><br>    <span class="hljs-keyword">int</span> den; <span class="hljs-comment">///&lt; denominator</span><br>&#125; AVRational;<br></code></pre></td></tr></table></figure><p>那么AVRational到底表示了什么呢？</p><p>AVRational这个结构标识一个分数，num为分子，den为分母。实际上time_base的意思就是时间的刻度。</p><p>如果把1秒分为25等份，你可以理解就是一把尺，那么每一格表示的就是1/25秒。此时的time_base={1，25}。如果你是把1秒分成90000份，每一个刻度就是1/90000秒，此时的time_base={1，90000}。所谓时间基表示的就是每个刻度是多少秒。<br>那么，在刻度为1/25的体系下的time=5，转换成在刻度为1/90000体系下的时间time为(5x1/25)/(1/90000) = 3600*5=18000</p><p>正是由于不同的封装格式，timebase是不一样的。另外，整个转码过程，不同的数据状态对应的时间基也不一致。所以在实际开发过程中，存在着大量时间基的转换。</p><h1 id="2-PTS和DTS"><a href="#2-PTS和DTS" class="headerlink" title="2.PTS和DTS"></a>2.PTS和DTS</h1><p><strong>PTS</strong>：Presentation Time Stamp。PTS 主要用于度量解码后的视频帧什么时候被显示出来。<br><strong>DTS</strong>：Decode Time Stamp。DTS 主要是标识读入内存中的Bit流在什么时候开始送入解码器中进行解码。</p><p><font color=#999AAA >虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。当视频流中没有 B 帧时，通常 DTS 和 PTS 的顺序是一致的。但如果有 B 帧时，解码顺序和播放顺序不一致了。</font></p><p>来看一个具体的例子，利用雷神做的videoeye视频码流分析软件，来对视频文件进行分析，这个文件是mp4格式的，可以看到视频码流PTS在递增，这就是我们看到画面的顺序，但是码流顺序并不是递增的，这里的码流顺序可理解为解码的顺序，也就是DTS表示的意思，先解I帧，再解P帧，依次解中间的B帧</p><p><img src="https://img-blog.csdnimg.cn/de079492fdf24da0a5dad99f0c8f281c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"></p><p>而音频的解码顺序就是我们依次听到的顺序，PTS和DTS相等</p><p><img src="https://img-blog.csdnimg.cn/7dbc87c806d54fb0a48a877a5e59996d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"></p><p>怎么理解PTS数值表达的含义呢，如果有某一帧，假设它是第10秒开始显示。那么它的pts是多少呢。是10？还是10s？还是两者都不是。</p><p><strong>这就引出了pts和dts的值到底代表了什么含义这个问题</strong></p><p>pts和dts的值指的是占多少个时间刻度（占多少个格子）。它的单位不是秒，而是时间刻度。只有pts与time_base两者结合在一起，才能表达出具体的时间是多少。好比我只告诉你，某个物体的长度占某一把尺上的20个刻度。但是我不告诉你，每个刻度是多少厘米，你仍然无法知道物体的长度。pts 就是这样的东西，pts(占了多少个时间刻度) ，time_base(每个时间刻度是多少秒) ，而帧的显示时间戳 =  pts(占了多少个时间刻度)  * time_base(每个时间刻度是多少秒)。</p><h1 id="3-一些开发过程中时间基转换的场景"><a href="#3-一些开发过程中时间基转换的场景" class="headerlink" title="3.一些开发过程中时间基转换的场景"></a>3.一些开发过程中时间基转换的场景</h1><p><strong>3.1 计算视频总时长</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c">AVFormatContext *ifmt_ctx = <span class="hljs-literal">NULL</span>;<br>avformat_open_input(&amp;ifmt_ctx, filename, <span class="hljs-literal">NULL</span>, <span class="hljs-literal">NULL</span>);<br><span class="hljs-keyword">int</span> totalMs;<span class="hljs-comment">//视频总毫秒数</span><br>totalMs = ifmt_ctx-&gt;duration / (AV_TIME_BASE / <span class="hljs-number">1000</span>);<br></code></pre></td></tr></table></figure><p><strong>3.2 根据PTS求出一帧在视频中对应的秒数位置</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">double</span> sec = enc_pkt.pts * av_q2d(ofmt_ctx-&gt;streams[stream_index]-&gt;time_base);<br></code></pre></td></tr></table></figure><p><strong>3.3 ffmpeg内部的时间戳与标准的时间转换方法</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">//timestamp为ffmpeg内部时间戳，time为正常时间戳，单位为秒</span><br>timestamp = AV_TIME_BASE * time<br>time = AV_TIME_BASE_Q * timestamp<br></code></pre></td></tr></table></figure><p>AV_TIME_BASE这个宏为1000000，由此我们可以发现ffmpeg内部时间戳是以微秒(μs)为单位的</p><p><strong>3.4 当需要把视频Seek到N秒的时候</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">//pos单位毫秒</span><br><span class="hljs-keyword">double</span> pos;<br>seekPos = ifmtctx-&gt;streams[videoStream]-&gt;duration * pos;<br>av_seek_frame(ifmtctx, videoStream, seekPos, AVSEEK_FLAG_BACKWARD | AVSEEK_FLAG_FRAME);<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>FFmpeg</category>
      
    </categories>
    
    
    <tags>
      
      <tag>音视频原理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WebRTC P2P技术——STUN、TURN和ICE</title>
    <link href="/2021/08/13/WebRTC-P2P%E6%8A%80%E6%9C%AF%E2%80%94%E2%80%94STUN%E3%80%81TURN%E5%92%8CICE/"/>
    <url>/2021/08/13/WebRTC-P2P%E6%8A%80%E6%9C%AF%E2%80%94%E2%80%94STUN%E3%80%81TURN%E5%92%8CICE/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将简单介绍WebRTC P2P技术中的STUN、TURN、和ICE协议。首先需要了解的是NAT协议。</p><h1 id="1-NAT"><a href="#1-NAT" class="headerlink" title="1.NAT"></a>1.NAT</h1><p>NAT是Net Address Translation的缩写，即网络地址转换。</p><p>家庭和办公网络环境大多是经过NAT路由中转的方式联网。这也意味着在家PC通过WIFI联网，在PC上通过命令行（ifconfig）查看到的IP地址（内网），跟通过baidu查看到的IP地址（公网），不一样，这也能证明PC处于NAT后面。</p><p>首先介绍一下NAT的分类以及NAT打洞原理。</p><h2 id="NAT分类"><a href="#NAT分类" class="headerlink" title="NAT分类"></a>NAT分类</h2><p><strong>完全锥形 Full Cone</strong></p><p><img src="https://img-blog.csdnimg.cn/74e7f306a1f641dbb4e161ef92cd9999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"><br>内部机器A访问外网机器C，NAT打开一个端口，后面外网的任意ip和任意port都可以访问这个端口，也就是任意ip+任意port可以访问内网机器A。</p><p>缺点：安全性不好，因为可以被任意ip和端口访问</p><p><strong>地址限制锥形 Address Restricted Cone</strong></p><p><img src="https://img-blog.csdnimg.cn/c21a7a184a5c406a9cc4f947e0da5cf7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"><br>​ 内部机器A访问外网机器C，NAT打开一个端口，后面机器C的任意port可以访问这个端口，就是只能固定ip+任意port访问A</p><p><strong>端口限制锥形 Port Restricted Cone</strong></p><p><img src="https://img-blog.csdnimg.cn/efb1c079313a4cc49c1bc9b9445ec3b0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"><br>内部机器A访问外网机器C，NAT打开一个端口，后面机器C的固定port可以访问这个端口，就是只能固定ip+固定port访问A</p><p><strong>对称型 Symmetric</strong></p><p><img src="https://img-blog.csdnimg.cn/e518eaa6349746e08951bedfc2fbb947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"><br>​连接不同的外部Server，NAT打开的端口会变化。也就是内部机器A连接外网机器B时，NAT会打开一个端口，连接外网机器C时又会打开另外一个端口。</p><h2 id="NAT穿越基本步骤"><a href="#NAT穿越基本步骤" class="headerlink" title="NAT穿越基本步骤"></a>NAT穿越基本步骤</h2><p><strong>C1，C2向STUN发消息</strong></p><p>C1C2向STUN发送消息，拿到各自的公网IP和端口</p><p><strong>交换公网IP和端口</strong></p><p>将C1的公网ip和端口发送给C2，C2的公网ip和端口发送给C1</p><p><strong>C1-&gt;C2,C2-&gt;C1,甚至是端口猜测</strong></p><p>C1C2就可以通信了，但是在对称型的场景下，甚至需要端口猜测</p><p><font color='red'> <strong>NAT穿越组合里，除了对称型与对称型无法打通以外，其余组合都可以打通</strong> </font></p><h1 id="2-STUN"><a href="#2-STUN" class="headerlink" title="2.STUN"></a>2.STUN</h1><p>STUN，首先在RFC3489中定义，作为一个完整的NAT穿透解决方案，英文全称是Simple Traversal of UDP Through NATs，即简单的用UDP穿透NAT。</p><p>在新的RFC5389修订中把STUN协议定位于为穿透NAT提供工具，而不是一个完整的解决方案，英文全称是Session Traversal Utilities for NAT，即NAT会话穿透效用。RFC5389与RFC3489除了名称变化外，最大的区别是支持TCP穿透。</p><p><img src="https://img-blog.csdnimg.cn/25f6c2b6417a4e59887ed058b804cf0d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"></p><p>STUN主要有3个功能，分别是检测是否位于NAT后面，检测NAT的类型，获取经过NAT转换后的地址和端口。</p><p>客户端通过给公网的 STUN 服务器发送请求获得自己的公网地址信息，以及是否能够穿过路由器访问。</p><p>但是一些路由器严格地限定了部分私网设备的对外连接。这种情况下，即使 STUN 服务器识别了该私网设备的公网 IP 和端口的映射，依然无法和这个私网设备建立连接。这种情况下就需要转向 TURN 协议。</p><h1 id="3-TURN"><a href="#3-TURN" class="headerlink" title="3.TURN"></a>3.TURN</h1><p>TURN协议是建立在UDP协议之上的一个应用层协议，首先在RFC5766中定义，英文全称是Traversal Using Relays around NAT:Relay Extensions to Session Traversal Utilities for NAT，即使用中继穿透NAT:STUN的扩展。</p><p><img src="https://img-blog.csdnimg.cn/e0000fe72b304a84936260bbfa79c31b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"></p><p>一些路由器使用一种“对称型 NAT”的 NAT 模型。这意味着路由器只接受和对端先前建立的连接（就是下一次请求建立新的连接映射）。</p><p>NAT 的中继穿越方式 Traversal Using Relays around NAT (TURN) 通过 TURN 服务器中继所有数据的方式来绕过“对称型 NAT”。你需要在 TURN 服务器上创建一个连接，然后告诉所有对端设备发包到服务器上，TURN 服务器再把包转发给你。很显然这种方式是开销很大的，所以只有在没得选择的情况下采用。</p><h1 id="4-ICE"><a href="#4-ICE" class="headerlink" title="4.ICE"></a>4.ICE</h1><p>综上所述，STUN的目的是为了进行P2P通信，通过提供反射地址（Server Reflexive Address）这种能力来使双方可以进行P2P通信，但是依赖NAT类型的不同，这种方式是有失败的概率的：比如双方都为对称型NAT或者一方为对称型，另一方为端口限制型。</p><p>因为有失败的可能性，所以单纯的依赖STUN协议提供的反射地址的话，需要事先探测出双方的NAT类型，假如发现是对称型的NAT，那么就不打洞了，而是直接中转。目前网络类型纷繁复杂，STUN协议在5389的时候去掉了NAT类型的判断的能力，因为越来越多的实践发现，在多层NAT下，类型的探测不总是有效的。而使用ICE的时候，不需要事先探测NAT类型。STUN还有一个作用是为ICE提供支持(对Binding的扩展)。</p><p>TURN协议的目的是为了保证通信双方百分之百能进行通信，就是在只知道反射地址而打洞失败的情况下的一种补充方案—–使用中继，使用中继方式百分之百能使得双方进行通信，只不过已经不是P2P的了，而且伴随而来的是转发效率的问题。不过这不要紧，因为该协议的目的就是保证双方肯定能通信，损失效率来保证了连同性。<br><img src="https://img-blog.csdnimg.cn/a64a1ff4ea354d15a663f039533e7bfd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70#pic_center"></p><p><strong>ICE协议的目的就是综合以上两种方案，通过通信双方互相发探测包，找出一种最合理，最廉价的可行路径。</strong></p><p>ICE首先探测内网地址，再探测STUN提供的反射地址，最后探测TURN协议的中继地址，反正最终目的就是探出一条路，内网地址不行用反射地址，反射地址不行，最后不得已情况下那就用中继地址。</p>]]></content>
    
    
    <categories>
      
      <category>RTC开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>WebRTC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FFmpeg学习—打印音视频Meta信息</title>
    <link href="/2021/06/26/FFmpegDemo%E2%80%94%E6%89%93%E5%8D%B0%E9%9F%B3%E8%A7%86%E9%A2%91Meta%E4%BF%A1%E6%81%AF/"/>
    <url>/2021/06/26/FFmpegDemo%E2%80%94%E6%89%93%E5%8D%B0%E9%9F%B3%E8%A7%86%E9%A2%91Meta%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="1-主要的API"><a href="#1-主要的API" class="headerlink" title="1. 主要的API"></a>1. 主要的API</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Print detailed information about the input or output format, such as</span><br><span class="hljs-comment"> * duration, bitrate, streams, container, programs, metadata, side data,</span><br><span class="hljs-comment"> * codec and time base.</span><br><span class="hljs-comment"> *</span><br><span class="hljs-comment"> * @param ic        the context to analyze</span><br><span class="hljs-comment"> * @param index     index of the stream to dump information about</span><br><span class="hljs-comment"> * @param url       the URL to print, such as source or destination file</span><br><span class="hljs-comment"> * @param is_output Select whether the specified context is an input(0) or output(1)</span><br><span class="hljs-comment"> */</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">av_dump_format</span><span class="hljs-params">(AVFormatContext *ic, <span class="hljs-keyword">int</span> index, <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> *url, <span class="hljs-keyword">int</span> is_output)</span></span>;<br></code></pre></td></tr></table></figure><h2 id="2-一个结构体"><a href="#2-一个结构体" class="headerlink" title="2. 一个结构体"></a>2. 一个结构体</h2><p>AVFormatContext，具体的结构体介绍可以看雷神这篇文章</p><p><a href="https://blog.csdn.net/leixiaohua1020/article/details/14214705">AVFormat结构体介绍</a></p><h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3.代码"></a>3.代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;libavutil/log.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;libavformat/avformat.h&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc,<span class="hljs-keyword">char</span>* argv[])</span></span><br><span class="hljs-function"></span>&#123;<br><br>    <span class="hljs-keyword">int</span> ret;<br>    AVFormatContext *fmt_ctx = <span class="hljs-literal">NULL</span>;<br><br>    av_log_set_level(AV_LOG_INFO);<br><br>    av_register_all();<br><br>    avformat_open_input(&amp;fmt_ctx,<span class="hljs-string">&quot;/opt/sucai/test.mp4&quot;</span>,<span class="hljs-literal">NULL</span>,<span class="hljs-literal">NULL</span>);<br><br>    <span class="hljs-keyword">if</span>(ret&lt;<span class="hljs-number">0</span>)&#123;<br>        av_log(<span class="hljs-literal">NULL</span>,AV_LOG_ERROR,<span class="hljs-string">&quot;Can&#x27;t open file: %s\n&quot;</span>,av_err2str(ret));<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    &#125;<br><br>    av_dump_format(fmt_ctx,<span class="hljs-number">0</span>,<span class="hljs-string">&quot;/opt/sucai/test.mp4&quot;</span>,<span class="hljs-number">0</span>);<br>    avformat_close_input(&amp;fmt_ctx);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="4-执行结果分析"><a href="#4-执行结果分析" class="headerlink" title="4.执行结果分析"></a>4.执行结果分析</h2><p>执行结果如下<br><img src="https://img-blog.csdnimg.cn/20210625235807535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">#0代表流的索引值<br>Metadata与FFmpeg版本有关</p><p>Stream#0,第一路视频流，编码格式为h265；分辨率1280x720；码率1008kb/s；帧率25帧/s；25  tbr代表帧率；12800 tbn代表文件层（st）的时间精度，即1S=12800，和duration相关；</p><p>Stream#1，第二路音频流，编码格式aac，采样频率48000Hz，双声道，码率192kb/s</p>]]></content>
    
    
    <categories>
      
      <category>FFmpeg</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FFmpegDemo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WebRTC概述</title>
    <link href="/2021/06/20/WebRTC%E6%A6%82%E8%BF%B0/"/>
    <url>/2021/06/20/WebRTC%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>WebRTC的出现使实时通信技术得以广泛应用。 WebRTC制定、实现了一套统一且完  整的实时通信标准，并将这套标准开源。这套标准包含了实时通信技术涉及的所有内容，  使用这套标准，开发人员无须关注音视频编解码、网络连接、传输等底层技术细节，可以  专注于构建业务逻辑，且这些底层技术是完全免费的。   </p><p>WebRTC统一了各平台的实时通信技术，大部分操作系统及浏览器都支持 WebRTC,  无须安装任何插件，就可以在浏览器端发起实时视频通话。WebRTC技术最初为Web打造，随着 WebRTC自身的演进，目前已经可以将其应用于各种应用程序。</p><h1 id="1-WebRTC的历史"><a href="#1-WebRTC的历史" class="headerlink" title="1.WebRTC的历史"></a>1.WebRTC的历史</h1><p>WebRTC( Web Real-Time- Communication)是一个谷歌开源项目，它提供了一套标准API,使Web应用可以直接提供实时音视频通信功能，不再需要借助任何插件。原生通信  过程采用P2P协议，数据直接在浏览器之间交互，理论上不需要服务器端的参与。  </p><p>“为浏览器、移动平台、物联网设备提供一套用于开发功能丰富、高质量的实时音视频  应用的通用协议”是WebRTC的使命。   WebRTC的发展历史如下。 </p><ul><li>2010年5月，谷歌收购视频会议软件公司GIPS,该公司在RTC编码方面有深厚的  技术积累。  </li><li>2011年5月，谷歌开源 WebRTC项目。  </li><li>2011年10月，W3C发布第一个 WebRTC规范草案。  </li><li>2014年7月，谷歌发布视频会议产品 Hangouts,该产品使用了 WebRTC技术。  </li><li>2017年11月， WebRTC进入候选推荐标准(Candidate Recommendation,cr  阶段。</li></ul><h1 id="2-WebRTC的技术架构"><a href="#2-WebRTC的技术架构" class="headerlink" title="2.WebRTC的技术架构"></a>2.WebRTC的技术架构</h1><p>从技术实现的角度讲，在浏览器之间进行实时通信需要使用很多技术，如音视频编解  码、网络连接管理、媒体数据实时传输等，还需要提供一组易用的API给开发者使用。这 些技术组合在一起，就是 WebRTC技术架构，如下图所示。<br><img src="https://img-blog.csdnimg.cn/20210620175852592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>WebRTC技术架构的顶层可以分为两个部分。一部分是Web API，一组JavaScript接口，由W3C维护，开发人员可以使用这些API在浏览器中创建实时通信应用程序。另一部分是适  用于移动端及桌面开发的 libwebrtc,即使用 WebRTC++源码在 Windows、 Android、ios  等平台编译后的开发包，开发人员可以使用这个开发包打造原生的 WebRTC应用程序。  </p><p>第二层是 WebRTC++API,它是 Web API和 libwebrtc的底层实现。该层包含了连接  管理、连接设置、会话状态和数据传输的API。基于这些API,浏览器厂商可以方便地加入  对 WebRTC的支持。</p><p>WebRTC规范里没有包含信令协议，这部分需要研发人员依据业务特点自行实现 。</p><p>WebRTC支持的音频编码格式有OPUS和G.711,同时还在音频处理层实现了回音消除  及降噪功能。 WebRTC支持的视频编码格式主要有VP8和H264(还有部分浏览器支持VP9  及H265格式), WebRTC还实现了 Jitter Buffer防抖动及图像增强等高级功能 。<br>在媒体传输层，WebRTC在UDP之上增加了3个协议。  </p><ul><li>数据包传输层安全性协议(DTLS)用于加密媒体数据和应用程序数据。</li><li>安全实时传输协议(SRTP)用于传输音频和视频流。</li><li>流控制传输协议(SCTP)用于传输应用程序数据。<br>WebRTC借助ICE技术在端与端之间建立P2P连接，它提供了一系列API,用于管理连接。 WebRTC还提供了摄像头、话筒、桌面等媒体采集API,使用这些API可以定制媒体流。  </li></ul><h1 id="3-WebRTC的兼容性"><a href="#3-WebRTC的兼容性" class="headerlink" title="3.WebRTC的兼容性"></a>3.WebRTC的兼容性</h1><p>据 caniuse.com统计，大部分浏览器都实现了对WebRTC的支持，各浏览器支持情况如下。</p><ul><li>Firefox版本22+   </li><li>Chrome版本23+   </li><li>Safari版本11+   </li><li>iOS Safari版本11+  </li><li>Edge版本15+</li><li>Opera版本18+   </li><li>Android Browser版本81+   </li><li>Chrome for Android版本84+   </li><li>Firefox for Android版本68+ </li><li>IE不支持</li></ul><p>Android和iOS原生应用都支持 WebRTC,可以使用原生SDK开发跨平台的 WebRTC 应用。   Android WebView自36版本之后，提供了对 WebRTC的支持，这意味可以使用 WebRTC  API开发 Android混合App注意，一些手机厂商对部分 Android版本里的 Web View进行了裁剪，导致不能使用 WebRTC,这时候下载并安装最新的 Web View即可。</p><p>iOS Web View目前还不支持 WebRTC,但是可以使 cordova的插件 cordova-plugin- iosrtc在混合App中使用 WebRTC。   </p><p>WebRTC目前处于活跃开发阶段，各个浏览器的实现程度不一样。为了解决兼容性的  问题，谷歌提供了 adapter库。  </p><p>在 GitHub上可以下载最新版本的 adapterjs库，地址如下所示。<br> <a href="https://github.com/webrtc/adapter/tree/master/release">https://github.com/webrtc/adapter/tree/master/release</a></p><p>将下载的文件放到Web服务器根目录，在Web应用中引用。   </p><script src="adapter. js"></script><h1 id="4-WebRTC的网络拓扑"><a href="#4-WebRTC的网络拓扑" class="headerlink" title="4.WebRTC的网络拓扑"></a>4.WebRTC的网络拓扑</h1><p>WebRTC规范主要介绍了使用ICE技术建立P2P的网络连接，即Mesh网络结构。在   WebRTC技术的实际应用中，衍生出了媒体服务器的用法。  </p><p>使用媒体服务器的场景，通常是因为P2P连接不可控，而使用媒体服务器可以对媒体  流进行修改、分析、记录等P2P无法完成的操作。实际上，如果我们把媒体服务器看作   WebRTC连接的另外一端，就很容易理解媒体服务器的工作原理了。媒体服务器是WebRTC在服务器端的实现，起到了桥梁的作用，用于连接多个 WebRTC客户端，并增加了额外的媒体处理功能。通常根据提供的功能将媒体服务器区分成MCU和SFU。</p><h2 id="4-1-Mesh网络结构"><a href="#4-1-Mesh网络结构" class="headerlink" title="4.1 Mesh网络结构"></a>4.1 Mesh网络结构</h2><p>Mesh是WebRTC多方会话最简单的网络结构。在这种结构中，每个参与者都向其他所有参与者发送媒体流，同时接收其他所有参  与者发送的媒体流。说这是最简单的网络结构，是因为它是Web  RTC原生支持的，无须媒体服务器的参与。Mesh网络结构如图所示。<br><img src="https://img-blog.csdnimg.cn/20210620192621836.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70,#pic_center" alt="在这里插入图片描述"></p><p>在Mesh网络结构中，每个参与者都以P2P的方式相互连接，数据交换基本不经过中央  服务器(部分无法使用P2P的场景，会经过TURN服务器)。由于每个参与者都要为其他参  与者提供独立的媒体流，因此需要N-个上行链路和N-1个下行链路。众多上行和下行链  路限制了参与人数，参与人过多会导致明显卡顿，通常只能支持6人以下的实时互动场景。由于没有媒体服务器的参与，Mesh网络结构难以对视频做额外的处理，不支持视频录制、视频转码、视频合流等操作。</p><h2 id="4-2-MCU网络结构"><a href="#4-2-MCU网络结构" class="headerlink" title="4.2 MCU网络结构"></a>4.2 MCU网络结构</h2><p>MCU( Multipoint Control Unit)是一种传统的中心化网络结构，参与者仅与中心的  MCU媒体服务器连接。MCU媒体服务器合并所有参与者的视频流，生成一个包含所有参与者画面的视频流，参与者只需要拉取合流画面，MCU网络结构如下图所示。这种场景下，每个参与者只需要1个上行链路和1个下行链路了。<br><img src="https://img-blog.csdnimg.cn/20210620193444881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70,#pic_center" alt="在这里插入图片描述"></p><p>与Mesh网络结构相比，参与者所在的终端压力要小很多，可以支持更多人同时在线进行音视频通信。但是MCU服务器负责所有视频编码、转码、解码、合流等复杂操作，服务器端压力较大，需要较高的配置。同时由于合流画面固定，界面布局也不够灵活。</p><h2 id="4-3-SFU网络结构"><a href="#4-3-SFU网络结构" class="headerlink" title="4.3 SFU网络结构"></a>4.3 SFU网络结构</h2><p>在SFU( Selective Forwarding Unit)网络结构中，仍然有中心节点媒体服务器，但是中心节点只负责转发，不做合流、转码等资源开销较大的媒体处理工作，所以服务器的压  力会小很多，服务器配置也不像MCU的要求那么高。每个参与者需要1个上行链路和N-1个下行链路，带宽消耗低于Mesh,但是高于MCU。<br><img src="https://img-blog.csdnimg.cn/20210620193612976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70,#pic_center" alt="在这里插入图片描述"></p><p>我们可以将SFU服务器视为一个 WebRTC参与方，它与其他所有参与方进行1对1的  建立连接，并在其中起到桥梁的作用，同时转发各个参与者的媒  体数据。SFU服务器具备复制媒体数据的能力，能够将一个参与  者的数据转发给多个参与者。SFU服务器与TURN服务器不同，  TURN服务器仅仅是为 WebRTC客户端提供的一种辅助数据转发通道，在无法使用P2P的情况下进行透明的数据转发，TURN服务器不具备复制、转发媒体数据的能力。  </p><p>下图为各解决方案的流量对比图<br><img src="https://img-blog.csdnimg.cn/20210620190228631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70,#pic_center" alt="在这里插入图片描述"><br>那么我们应该使用哪种架构呢？</p><p>这个就需要根据自己的项目的需要了。其实，商业解决方案，包括上述所有方案，往往需要根据客户的实际应用场景选择对于的方法。不过，也有经验，你可以使用一些通用规则。</p><ol><li>如果你仅是提供P2P音视频传输的服务，Mesh架构可能是最适合你的。另外，如果基础设施的成本不是问题，并且参与者具有异构连接，这可以是一个很好的解决方案。</li><li>假设你提供企业级服务，且有较好的宽带和高效的硬件（即一个企业内部服务），参加人数是有限的，那么你非常适合MCU方案。</li><li>一般来说，如果你提供大规模服务的，应优先考虑到SFU的方案。Router传输接近把情报在网络的边界，构建最终用户应用程序时，以达到更好的可扩展性和灵活性的网络的范例</li></ol><p><strong>Simulcast联播</strong>  </p><p>在进行 WebRTC多方视频会话时，参与人数较多，硬件设施、网络环境均有差异，这种情况下如何确保会话质量呢?使用MCU时，这个问题相对简单一些。MCU可以根据参与者的网络质量和设备能力，提供不同的清晰度和码率。但是随之而来的问题是服务器资源压力较大，难以支撑大规模并发，同时也显著增加了使用成本。</p><p>多人会话场景选择SFU网络结构是目前通用的做法。早期的SFU只是将媒体流从发送  端转发给接收端，无法独立为不同参与者调整视频码率，其结果是发送者需要自行调整码  率，以适应接收条件最差的参与者。而那些网络环境较好的参与者只能接收相同质量的媒  体流，别无选择。</p><p>Simulcast技术对SFU进行了优化，发送端可以同时发送多个不同质量的媒体流给接收  端。SFU能够依据参与者的网络质量，决定转发给参与者哪种质量的媒体流。因为发送者需要发送多个不同质量的媒体流所以会显著增加发送设备的载荷，同时占用发送者上行带宽资源。</p><p><strong>可伸缩视频编码</strong></p><p>可伸缩视频编码( Scalable Video Coding,SVC)是 Simulcast的改进技术。它使用分层编码技术，发送端只需要发送一个独立的视频流给SFU,SFU根据不同的层，解码出不同  质量的视频流，并发送给不同接收条件的参与者。</p><p>SVC中多个层次的媒体流相互依赖，较高质量的媒体数据需要较低质量的媒体数据解码。SFU接收到SVC编码的内容后，根据客户端的接收条件选择不同的编码层次，从而获得不同质量的媒体流。</p><p>如果媒体流包括多个不同分辨率的层，则称该编码具有空间可伸缩性；如果媒体流包  含多个不同帧率的层，则称该编码具有时间可伸缩性；如果媒体流包含多个不同码率的层，则称该编码具有质量可伸缩性。</p><p>在编码空间、时间、质量均可伸缩的情况下，SFU可以生成不同的视频流，以适应不同客户端的接收条件。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文仅对WebRTC作一个简单的介绍，具体参考了栗伟老师的书《WebRTC技术详解》以及其他的一些博客，在此表示感谢。</p>]]></content>
    
    
    <categories>
      
      <category>RTC开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>WebRTC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>H264编码原理</title>
    <link href="/2021/06/20/H264%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86/"/>
    <url>/2021/06/20/H264%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将从H264的发展历史开始，详细介绍H264编码的原理以及涉及的知识点，作为个人学习的总结。</p><h1 id="一、H264概述"><a href="#一、H264概述" class="headerlink" title="一、H264概述"></a>一、H264概述</h1><p>对于未入门的人可能会对命名有点疑惑,比如对于H.264, 还会看到有些标签还写成MPEG-4/AVC，这是什么原因呢？</p><p>可以简单理解为视频编码格式的制定主要有两大门派，本来H.26X系统由ITU-T主导开发, MPEG系列由ISO主导开发，然后两大门派合作开发了H.264 和H.265 , H.264,H.265是ITU组织对着两种编码格式的命名, MPEG-4/AVC ,MPEG-4/HEVC是ISO组织对这两种编码格式的命名。以下的图片展示了视频编码格式的发展。<br><img src="https://img-blog.csdnimg.cn/20210619163700484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p><h1 id="二、H264压缩码率和GOP"><a href="#二、H264压缩码率和GOP" class="headerlink" title="二、H264压缩码率和GOP"></a>二、H264压缩码率和GOP</h1><h2 id="压缩比和码率设置"><a href="#压缩比和码率设置" class="headerlink" title="压缩比和码率设置"></a>压缩比和码率设置</h2><p>原始YUV数据像素格式为YUV420，分辨率为640x480，帧率为15帧，计算得到原始视频数据码流为55Mbps左右，而H264建议的码流大小为500kbps，结果显示，H264压缩比达到1/100左右！</p><p>具体码流大小的设定可参考声网的技术文档，如下图，链接<a href="https://docs.agora.io/cn/Video/video_profile_android?platform=Android">声网文档资料</a><br><img src="https://img-blog.csdnimg.cn/20210619165835831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="GOP和I帧，P帧，B帧"><a href="#GOP和I帧，P帧，B帧" class="headerlink" title="GOP和I帧，P帧，B帧"></a>GOP和I帧，P帧，B帧</h2><p>GOP即Group of picture（图像组），可以理解为GOP是一组连续的图像，并且他们之间相关性很大，在一组GOP中包含I帧，B帧和P帧。<br>对每一帧的类型又可以划分为：</p><p>·I帧：关键帧，采用帧内压缩技术。<br>·P帧：向前参考帧，在压缩时，只参考前面已经处理的帧。采用帧压缩技术。大小大约为I帧的一半。<br>·B帧：双向参考帧，在压缩时，它即参考前而的帧，又参考它后面的帧。采用帧间压缩技术。大小约为I帧大小的1/4。</p><p>短视频采用很多b帧和p帧，I帧很少，相应的GOP序列很长，这就是在弱网络环境下依然可以播放流畅的原因。</p><p><img src="https://img-blog.csdnimg.cn/20210619160939925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>IDR帧是什么？</strong></p><p>IDR帧为解码器立刻刷新帧，每当遇到IDR帧时，解码器会立刻清空参考buffer中的内容，IDR帧的作用是使错误不致传播,从IDR帧开始,重新算一个新的序列开始编码，IDR帧可以随机访问，而I帧不具有随机访问的能力，也就说IDR帧是一种特殊的I帧，但I帧不一定是IDR帧。</p><p><font color='red'> 疑问：既然如此，那I帧还有存在的必要吗？为什么不都叫IDR帧呢？ </font></p><p>通过查阅资料发现，上图展示的GOP仅仅是一种情况而已，对于存在大量帧的GOP，如果仅依赖第一个IDR帧，则可能会造成解码出与原始图像差异较大的情况；编码器会在适当的时候（如场景切换）强制插入一个I帧来提高解码图像的质量。<br>在每个IDR帧前面都有两个帧，SPS和PPS。（在编解码流程时候再具体分析）<br><strong>·SPS（Sequence Parameter Set）</strong><br>序列参数集，作用于一串连续的视频图像。如seq_parameter_set_id，帧数以及POC（picture order count）的约束、参考帧数目、解码图像尺寸和帧场编码模式选择标识等<br><strong>·PPS（Picture Parameter Set）</strong><br>图像参数集，作用于视频序列中的图像。如pic_parameter_set_id、熵编码模式选择标识、片组数目、初始量化参数和去方块铝箔系数调整标识等。</p><p><strong>H264编码规则</strong><br>有了上述的知识，对于I帧，P帧和B帧就有了一个认识，即是什么，那为什么呢，为什么编码器要将他们分为I帧P帧和B帧来编码呢，编码器是根据什么规则来确定他们是I帧P帧和B帧的呢？</p><p>在相邻几个帧中，一般有差别的像素只有10%以内的点亮度差值变化不超过2%，而色度变化只有1%以内，所以对于一段变化不大的图像画面，我们可以先编码出一个完整的图像帧，即I帧为完整的图像帧。<br>与I帧相似程度高达95%以上的帧编码为B帧，相似程度70%编码成P帧，利用libx264这个工具帮我们完成。</p><h1 id="二、H264压缩技术"><a href="#二、H264压缩技术" class="headerlink" title="二、H264压缩技术"></a>二、H264压缩技术</h1><p>H264压缩技术主要采用了以下几种方法对视频数据进行压缩。包括：</p><p>·帧内预测压缩，解决的是空域数据冗余问题。<br>·帧间预测压缩（运动估计与补偿），解决的是时域数据冗余问题。<br>·无损压缩，利用哈夫曼编码的原理对视频进一步压缩，得到更大的压缩比。</p><h2 id="1-H264中的宏块划分"><a href="#1-H264中的宏块划分" class="headerlink" title="1.H264中的宏块划分"></a>1.H264中的宏块划分</h2><p>·宏块是视频压缩的操作的基本单元，无论是帧内压缩，都以宏块为单位<br>H264宏块划分，以下面这幅原始图片为例：<br><img src="https://img-blog.csdnimg.cn/20210619161654562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>将上面的图片可以划分成很多的宏块<br><img src="https://img-blog.csdnimg.cn/20210619225136202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>H264的16x16的宏块可以继续划分，划分为更小的子块<br><img src="https://img-blog.csdnimg.cn/20210619230408649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210619230637791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-帧内压缩（I帧）"><a href="#2-帧内压缩（I帧）" class="headerlink" title="2.帧内压缩（I帧）"></a>2.帧内压缩（I帧）</h2><p>帧内压缩是利用一个帧内相邻像素差别不大的特点，所以可以进行<strong>宏块预测</strong>，人们对亮度的敏感度超过色度，YUV数据天然就可以将亮度与色度分开。</p><p>下图展示了帧内预测的9中模式和帧内预测的结果<br><img src="https://img-blog.csdnimg.cn/20210619231211562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>具体的预测模式的如下图所示，根据已知的宏块信息去推测下未知的宏块信息。<br><img src="https://img-blog.csdnimg.cn/20210619231431442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>不同模式预测的结果是有差异性的</p><p><img src="https://img-blog.csdnimg.cn/20210619231554150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>依据与原始数据最相似的结果来确定预测的模式，通过宏块的预测可以节省数据存储的空间，但是预测的结果与原图并不是100%相同的，如图所示</p><p><img src="https://img-blog.csdnimg.cn/20210619231810822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>将原图与预测结果进行差值计算，得到残差值，也就是说，预测结果+残差值=原图<br><img src="https://img-blog.csdnimg.cn/20210619231939225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对预测结果和残差值进行进一步压缩，是小于原始图像的，这就达到了帧内压缩的目的。<br><img src="https://img-blog.csdnimg.cn/20210619232219719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="3-帧间压缩（P帧，B帧）"><a href="#3-帧间压缩（P帧，B帧）" class="headerlink" title="3.帧间压缩（P帧，B帧）"></a>3.帧间压缩（P帧，B帧）</h2><p>帧间压缩是利用一个GOP内相邻帧强相关的特点，通过后面的帧参考前面的帧的方法，通过宏块匹配的方法，找到宏块的运动矢量，如下图所示的GOP中不同帧之间的差别就是小球的位置。<br><img src="https://img-blog.csdnimg.cn/20210619235100947.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对于相邻帧之间的差别，通过宏块的查找，也就是运动估计，得到每一帧的运动矢量，如下图右边所示，只需存储很少的数据量即可得到每一帧之间的差别，可以根据参考帧还原回原始的图像</p><p><img src="https://img-blog.csdnimg.cn/20210619235446606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>同样地，在运动估计的宏块查找过程中，并不是100%的能找到一样的宏块，因此存在一定的误差，因此记录下残差值，将运动矢量+残差值即可解码得到完整的图像<br><img src="https://img-blog.csdnimg.cn/20210619235743454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><font color='red'> 在直播或者音视频通话中，会出现视频画屏或者卡顿的的情况，这是什么原因呢？</font></p><p>花屏的原因：GOP中有帧丢失，残差值和运动矢量丢失，造成了模糊的数据，解码端图像发生了错误，因此会出现马赛克（花屏）。</p><p>卡顿的原因：为了避免花屏的发生，当发现有帧丢失的时候，就丢弃GOP中所有的帧，直到下一个IDR帧重新刷新图像。</p><p>根据具体的产品的应用场景选择丢帧的解决策略：例如，音视频会议，允许花屏，但总体流畅就OK，共享桌面花屏使得用户观感体验不好，无法看清内容，允许卡顿，但不要花屏。</p><h2 id="4-无损压缩"><a href="#4-无损压缩" class="headerlink" title="4.无损压缩"></a>4.无损压缩</h2><p><strong>整数离散余弦变换（DCT）</strong>，将空间上的相关性变为频域上无关的数据然后进行量化，如图所示，将8x8的宏块数据变换到到角上，方便后续的无损压缩<br><img src="https://img-blog.csdnimg.cn/20210620001013536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20210620001111725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>CABAC压缩</strong></p><p>压缩的原理是利用哈夫曼编码，使用频率高的数据就用小的数据量表示，使用频率低的数据就用大的数据量表示，如图所示为Mpeg2VLC压缩的原理。<br><img src="https://img-blog.csdnimg.cn/20210620001454105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">        </p><p>在此基础上，CABAC，采用上下文适配的策略，有点类似于帧间压缩参考帧的原理，随着时间的推移，充分利用上下文信息，数据块越来越小，进一步压缩了数据量。</p><p><img src="https://img-blog.csdnimg.cn/20210620001257827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文只是对H264编码的原理进行了阐释，具体的H264编解码的流程和码流信息，以及H264的分析将会在后面的文章中提到，文中的图片来自于慕课网李超老师的课程以及其他博客的学习资料，在此表示感谢。</p>]]></content>
    
    
    <categories>
      
      <category>音视频基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>H264</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FFMpeg命令分类</title>
    <link href="/2021/06/04/article2/"/>
    <url>/2021/06/04/article2/</url>
    
    <content type="html"><![CDATA[<p>本文主要介绍FFmpeg的命令分类及基本的使用，具体请参照《FFmpeg从入门到精通》</p><p><img src="https://img-blog.csdnimg.cn/20210603225216334.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="FFmpeg命令分类">  </p><hr><h2 id="1-信息查询命令"><a href="#1-信息查询命令" class="headerlink" title="1.信息查询命令"></a>1.信息查询命令</h2><p><img src="https://img-blog.csdnimg.cn/20210603225327708.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="基本信息查询命令列表"></p><h2 id="2-录制命令"><a href="#2-录制命令" class="headerlink" title="2.录制命令"></a>2.录制命令</h2><p>参数说明<br>-f 设备驱动<br>-i 设备编号</p><h4 id="采集摄像头："><a href="#采集摄像头：" class="headerlink" title="采集摄像头："></a>采集摄像头：</h4><p>根据采集过程中终端中的提示信息按照规定的格式播放。 我的摄像头默认是640x480，25帧，像素格式是yuyv422，使用ffplay播放需要设定正确的参数</p><p>ffmpeg -f video4linux2 -i /dev/video0 out.yuv</p><h4 id="采集桌面："><a href="#采集桌面：" class="headerlink" title="采集桌面："></a>采集桌面：</h4><p>ffmpeg -f x11grab -framerate 25 -video_size 1920x1080 -i :0.0 out.mp4</p><h4 id="采集音频"><a href="#采集音频" class="headerlink" title="采集音频"></a>采集音频</h4><p>ffmpeg -f alsa -i hw:0 out.wav</p><h2 id="3-分解与复用命令"><a href="#3-分解与复用命令" class="headerlink" title="3.分解与复用命令"></a>3.分解与复用命令</h2><p>ffmpeg -i in.mp4 -vcodec copy -acodec copy out.flv</p><p>-an 不要音频数据<br>-vn 不要视频数据</p><h2 id="4-处理原始数据命令"><a href="#4-处理原始数据命令" class="headerlink" title="4.处理原始数据命令"></a>4.处理原始数据命令</h2><h4 id="提取yuv数据："><a href="#提取yuv数据：" class="headerlink" title="提取yuv数据："></a>提取yuv数据：</h4><p>ffmpeg -i input.mp4 -an -c:v rawvideo -pix_fmt yuv420p out.yuv</p><p>-c:v 对视频编码，用原始视频进行编码</p><h4 id="提取PCM数据："><a href="#提取PCM数据：" class="headerlink" title="提取PCM数据："></a>提取PCM数据：</h4><p>ffmpeg -i input.mp4 -vn -ar 44100 -ac2 -f s16le out.pcm</p><p>-ar 音频采样率<br>-ac 声道数<br>-f 数据存储方式</p><h2 id="5-滤镜命令"><a href="#5-滤镜命令" class="headerlink" title="5.滤镜命令"></a>5.滤镜命令</h2><h4 id="视频尺寸修改滤镜"><a href="#视频尺寸修改滤镜" class="headerlink" title="视频尺寸修改滤镜"></a>视频尺寸修改滤镜</h4><p>ffmpeg -i in.mp4 -vf crop=in_w-200:in_h-200 -c:v libx264 -c:a copy out.mp4</p><h2 id="6-裁剪与合并"><a href="#6-裁剪与合并" class="headerlink" title="6.裁剪与合并"></a>6.裁剪与合并</h2><h4 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h4><p>ffmpeg -i in.mp4 -ss 00:00:00 -t 10 out.mp4</p><p>-ss 裁剪起始时间<br>-t   裁剪时长</p><h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><p>ffmpeg -f concat -i inputs.txt out.flv</p><p>-f concat 对后面的文件进行拼接 inputs.txt 内容为 ‘file filename’格式。file内容如下，和裁切片段一个目录下，采用相对路径<br><img src="https://img-blog.csdnimg.cn/20210604124948452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="file文件内容"></p><h2 id="7-图片与视频命令互转"><a href="#7-图片与视频命令互转" class="headerlink" title="7.图片与视频命令互转"></a>7.图片与视频命令互转</h2><h4 id="视频转图片"><a href="#视频转图片" class="headerlink" title="视频转图片"></a>视频转图片</h4><p> ffmpeg -i outlydy2.mp4 -r 1 -f image2 image-%3d.jpeg</p><p>-r 1 1秒钟一张图片<br>-f image2图片格式 </p><p><img src="https://img-blog.csdnimg.cn/2021060412573361.png" alt="转图片结果"></p><h4 id="图片转视频"><a href="#图片转视频" class="headerlink" title="图片转视频"></a>图片转视频</h4><p>ffmpeg -i image-%3d.jpeg out.mp4 ffplay -r 1 hello.mp4</p><h2 id="8-直播推-拉流"><a href="#8-直播推-拉流" class="headerlink" title="8.直播推/拉流"></a>8.直播推/拉流</h2><h4 id="推流"><a href="#推流" class="headerlink" title="推流"></a>推流</h4><p>ffmpeg -re -i out.mp4 -c copy -f flv rtmp://127.0.0.1/live/livestream</p><p>-re 按时间戳读取文件</p><h4 id="拉流"><a href="#拉流" class="headerlink" title="拉流"></a>拉流</h4><p>拉到本地保存成多媒体文件 </p><p>ffmpeg -i  rtmp://127.0.0.1/live/livestream -c copy live.flv</p><p>ffplay直接观看 </p><p>ffplay rtmp://127.0.0.1/live/livestream</p>]]></content>
    
    
    <categories>
      
      <category>FFmpeg</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FFmpeg命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FFmpeg编译与依赖库安装</title>
    <link href="/2021/06/03/post1/"/>
    <url>/2021/06/03/post1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>刚开始学音视频开发，环境的配置研究了几天在几个虚拟机上都尝试了一下，终于把刚开始能踩的坑都踩遍了，遇到了各种问题，经过我的收集整理，把所有遇到的问题都放进来，后面再遇到问题还会继续更新的，作为个人以后参考用，如果写的有错误请各位大佬在评论区指正！感谢很多人帮助了我，希望自己也可以帮助更多的人。<br>我是Ubuntu18.04的版本，不知道是不是版本问题，下的版本好多配置和包都没有，手动配置了好久。</p><h1 id="前期准备工作"><a href="#前期准备工作" class="headerlink" title="前期准备工作"></a>前期准备工作</h1><p>省得之后文件权限问题，都是血和泪的教训啊，记不清在哪个错误的目录下 chmod -R -777 结果sudo su打不开了，为了防止和我一样的错误，先设置一下默认进入root用户，及时备份虚拟机。<br>具体的看这篇，很详细了，感谢这位老哥<br><a href="https://blog.csdn.net/qq_39591507/article/details/81288644">https://blog.csdn.net/qq_39591507/article/details/81288644</a><br>以root身份登陆以后打开shell是这样的<br><img src="https://img-blog.csdnimg.cn/20210420150510445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">下一步，打开根目录下opt文件夹，新建一个lib文件夹，之后编译的库放里面，后面全部编译完成以后把这个文件夹备份一下<br><img src="https://img-blog.csdnimg.cn/20210420150955569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">里面我已经编译好了，下载的库资源全部放这里面。</p><h1 id="安装yasm"><a href="#安装yasm" class="headerlink" title="安装yasm"></a>安装yasm</h1><p>直接在刚刚的lib文件夹下面打开终端<br>wget <a href="http://www.tortall.net/projects/yasm/releases/yasm-1.3.0.tar.gz">http://www.tortall.net/projects/yasm/releases/yasm-1.3.0.tar.gz</a><br>tar xvzf yasm-1.3.0.tar.gz<br>cd yasm-1.3.0<br>./configure<br>make &amp;&amp; make install</p><p>PS：//默认的./configure 配置是安装在usr/local/lib里面，后面ffmpeg不放进去，前期的一些必要的组件直接configure没问题。</p><h1 id="安装nasm（2-13以上版本）"><a href="#安装nasm（2-13以上版本）" class="headerlink" title="安装nasm（2.13以上版本）"></a>安装nasm（2.13以上版本）</h1><p>和上面一样<br>wget <a href="https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2">https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2</a><br>tar xvf nasm-2.14.02.tar.bz2<br>cd nasm-2.14.02<br>./configure<br>make<br>make install</p><h1 id="安装其他依赖"><a href="#安装其他依赖" class="headerlink" title="安装其他依赖"></a>安装其他依赖</h1><p>apt install cmake -y<br>apt install pkg-config   //后面编译x264和x265需要</p><h1 id="编译x264（只编译静态库）"><a href="#编译x264（只编译静态库）" class="headerlink" title="编译x264（只编译静态库）"></a>编译x264（只编译静态库）</h1><p>x264下载地址：<br><a href="http://ftp.videolan.org/pub/videolan/x264/snapshots/">http://ftp.videolan.org/pub/videolan/x264/snapshots/</a><br>tar xvf x264-snapshot-20191024-2245-stable.tar.bz2<br>cd x264-snapshot-20191024-2245-stable<br>./configure –enable-static –prefix=../x264 –enable-pic<br>make -j<br>make install<br>//make -j4就是开启4个并行编译，make -j不加数字就是默认全部核心数，编译速度差不多要快一半<br>##编译x265（只编译静态库）<br>x265下载地址:<br><a href="http://ftp.videolan.org/pub/videolan/x265/">http://ftp.videolan.org/pub/videolan/x265/</a><br>tar xvf x265_3.2.tar.gz<br>//<strong>和x264编译不一样，264直接在根目录下编译，265要进入这个目录</strong><br>cd x265_3.2/build/linux/        </p><p>cmake -G “Unix Makefiles” -DCMAKE_INSTALL_PREFIX=”../../../x265” -DENABLE_SHARED:bool=off ../../source<br>make -j<br>make install<br>为了避免后面的问题，我们先进行如下的设置<br><img src="https://img-blog.csdnimg.cn/20210420153019777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>进入265这个目录，编辑一下这个文件<br>按如图添加依赖<br><img src="https://img-blog.csdnimg.cn/20210420153143863.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="编译SDL2"><a href="#编译SDL2" class="headerlink" title="编译SDL2"></a>编译SDL2</h1><p>编译之前检查一下Ubuntu是否声音输出是否正常，我的是不正常的，基本各种疑难杂症都给我遇见了，不过问题不大，参考一下别人的解决方法。<br><a href="https://blog.csdn.net/multimicro/article/details/82528730">https://blog.csdn.net/multimicro/article/details/82528730</a><br>确保有声音再进入下一步<br>我需要用到ffplay, 需要再下载SDL的源码，<a href="http://libsdl.org/release/">http://libsdl.org/release/</a><br>我的版本是SDL2-2.0.14<br>在编译之前先安装<br>sudo apt-get install libx11-dev<br>sudo apt-get install xorg-dev<br>不然会无法渲染SDL displa<br>如运行ffplay时，有些机器上会出现<br>Could not initialize SDL - No available video device<br>(Did you set the DISPLAY variable?)<br>说明系统中没有安装x11的库文件，因此编译出来的SDL库实际上不能用。</p><p>需要先编译安装SDL，和上面一样，直接默认编译安装</p><p>tar zxvf SDL2-2.0.8.tar.gz</p><p>cd SDL2-2.0.8</p><p>./configure</p><p>make</p><p>make install</p><h1 id="编译ffmpeg"><a href="#编译ffmpeg" class="headerlink" title="编译ffmpeg"></a>编译ffmpeg</h1><p>编译ffmpeg是最后一步，但是前面任何一步配置错误，回去修改以后，ffmpeg都要重新编译。</p><p>ffmpeg直接去官网下载，我下的是4.3.2版本的<br>解压以后还是在lib文件夹里面，我在官网下载的提取出来还套了一个文件夹，直接移到外面来，如图，不然没法直接执行下面的命令<br><img src="https://img-blog.csdnimg.cn/20210420155055789.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在当前页面打开终端执行<br>export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:../x264/lib/pkgconfig:../x265/lib/pkgconfig</p><p>./configure –enable-shared –enable-nonfree –enable-gpl –enable-pthreads –enable-libx264 –enable-libx265 –prefix=../ffmpeg </p><p>make -j<br>make install</p><p>编译完以后lib文件夹多了这三个</p><p><img src="https://img-blog.csdnimg.cn/20210420165525566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="编译ffmpeg-1"><a href="#编译ffmpeg-1" class="headerlink" title="编译ffmpeg"></a>编译ffmpeg</h1><p>编译完成后，进入/etc/profile中将ffmpeg加入到环境变量（在文件最后加上export PATH=/opt/lib/ffmpeg/bin:$PATH）<br><img src="https://img-blog.csdnimg.cn/20210420163102945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">进一步，把ffmpeg的库加入/etc/ld.so.conf中<br><img src="https://img-blog.csdnimg.cn/20210420163524174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="测试是否安装成功"><a href="#测试是否安装成功" class="headerlink" title="测试是否安装成功"></a>测试是否安装成功</h1><p>执行ffmpeg -devices<br><img src="https://img-blog.csdnimg.cn/20210420160325660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1Z2VidWN1bw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">我已经解决了，如果需要alsa驱动的需要装一下alsa驱动</p><p>之前的问题是编译完了ffmpeg发现没有alsa音频驱动，关于如何安装alsa音频驱动建议看这篇<br><a href="https://blog.csdn.net/keepingstudying/article/details/7373246">https://blog.csdn.net/keepingstudying/article/details/7373246</a></p><p>安装完了重启，然后重新编译ffmpeg<br>再测试一下ffplay命令，<br>ffplay xxx.MP4<br>如果可以正常播放视频,声音正常就ok，否则需要重装SDL2，然后重新编译ffmpeg</p><p>主体参考了云天之巅博主的，感谢！<br>具体不清楚的可以看云天之巅博主的视频<br><a href="https://www.bilibili.com/video/BV1Sz411v7Wm">https://www.bilibili.com/video/BV1Sz411v7Wm</a></p><hr>]]></content>
    
    
    <categories>
      
      <category>FFmpeg</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发环境搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
